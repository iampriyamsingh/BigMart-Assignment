{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b7175f3",
   "metadata": {},
   "source": [
    "## Using Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "743e17a9",
   "metadata": {},
   "source": [
    "### 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0c774c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c387ebef",
   "metadata": {},
   "source": [
    "### 2. Comparing XGBoost, Random Forest, Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0f354cba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading enhanced preprocessed training and test data...\n",
      "XGBoost with sklearn wrapper is not available, skipping this model.\n",
      "\n",
      "Starting K-Fold Cross-Validation for each model...\n",
      "\n",
      "Training and evaluating RandomForest...\n",
      "-> RandomForest completed. Mean RMSE: 1237.7706\n",
      "\n",
      "Training and evaluating LinearRegression...\n",
      "-> LinearRegression completed. Mean RMSE: 1121.5911\n",
      "\n",
      "Best model: LinearRegression with a mean RMSE of 1121.5911\n",
      "Training the best model on the full training data...\n",
      "Predictions generated.\n",
      "\n",
      "Creating submission file 'submission_best_model.csv'...\n",
      "Submission file 'submission_best_model.csv' created successfully!\n"
     ]
    }
   ],
   "source": [
    "# --- Load Preprocessed Data ---\n",
    "# We are loading the enhanced preprocessed data.\n",
    "print(\"Loading enhanced preprocessed training and test data...\")\n",
    "train_df = pd.read_csv('preprocessed_train_v2.csv')\n",
    "test_df = pd.read_csv('preprocessed_test_v2.csv')\n",
    "\n",
    "# Also load the original test identifiers to build the submission file.\n",
    "original_test_identifiers = pd.read_csv('test_AbJTz2l.csv')[['Item_Identifier', 'Outlet_Identifier']]\n",
    "\n",
    "# --- Define Features and Target ---\n",
    "y_train_log = np.log1p(train_df['Item_Outlet_Sales'])\n",
    "X_train = train_df.drop('Item_Outlet_Sales', axis=1)\n",
    "X_test = test_df.copy()\n",
    "\n",
    "# --- Define Models to Compare ---\n",
    "# We will compare a few different types of regression models.\n",
    "models = {}\n",
    "\n",
    "# We will use a try-except block to gracefully handle the ImportError with XGBoost.\n",
    "try:\n",
    "    models['XGBoost'] = xgb.XGBRegressor(objective='reg:squarederror', n_estimators=1000, learning_rate=0.05,\n",
    "                                    max_depth=5, subsample=0.8, colsample_bytree=0.8, random_state=42)\n",
    "except ImportError:\n",
    "    print(\"XGBoost with sklearn wrapper is not available, skipping this model.\")\n",
    "    pass\n",
    "\n",
    "models['RandomForest'] = RandomForestRegressor(n_estimators=1000, max_depth=5, min_samples_leaf=100,\n",
    "                                     n_jobs=-1, random_state=42)\n",
    "models['LinearRegression'] = LinearRegression()\n",
    "\n",
    "\n",
    "# --- K-Fold Cross-Validation ---\n",
    "# We will use 5-fold cross-validation to compare the models fairly.\n",
    "n_splits = 5\n",
    "kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "# Dictionary to store the mean RMSE for each model.\n",
    "rmse_scores = {}\n",
    "trained_models = {}\n",
    "\n",
    "print(\"\\nStarting K-Fold Cross-Validation for each model...\")\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"\\nTraining and evaluating {name}...\")\n",
    "    fold_scores = []\n",
    "    \n",
    "    # Iterate through each fold\n",
    "    for train_index, val_index in kf.split(X_train):\n",
    "        # Split the data into training and validation sets for this fold.\n",
    "        X_train_fold, X_val_fold = X_train.iloc[train_index], X_train.iloc[val_index]\n",
    "        y_train_log_fold, y_val_log_fold = y_train_log.iloc[train_index], y_train_log.iloc[val_index]\n",
    "        \n",
    "        # Train the model on the current fold's training data.\n",
    "        model.fit(X_train_fold, y_train_log_fold)\n",
    "        \n",
    "        # Make predictions on the validation data.\n",
    "        val_preds_log = model.predict(X_val_fold)\n",
    "        \n",
    "        # Inverse transform the predictions to the original scale to calculate RMSE.\n",
    "        val_preds = np.expm1(val_preds_log)\n",
    "        y_val = np.expm1(y_val_log_fold)\n",
    "        \n",
    "        # Calculate the Root Mean Squared Error (RMSE) for this fold.\n",
    "        rmse = np.sqrt(mean_squared_error(y_val, val_preds))\n",
    "        fold_scores.append(rmse)\n",
    "    \n",
    "    # Store the mean RMSE and the final trained model.\n",
    "    rmse_scores[name] = np.mean(fold_scores)\n",
    "    trained_models[name] = model\n",
    "\n",
    "    print(f\"-> {name} completed. Mean RMSE: {rmse_scores[name]:.4f}\")\n",
    "\n",
    "# --- Select the Best Model and Make Predictions ---\n",
    "# Find the model with the lowest (best) mean RMSE.\n",
    "best_model_name = min(rmse_scores, key=rmse_scores.get)\n",
    "\n",
    "print(f\"\\nBest model: {best_model_name} with a mean RMSE of {rmse_scores[best_model_name]:.4f}\")\n",
    "\n",
    "# Train the best model on the entire dataset one last time.\n",
    "print(\"Training the best model on the full training data...\")\n",
    "best_model = trained_models[best_model_name]\n",
    "best_model.fit(X_train, y_train_log)\n",
    "test_predictions_log = best_model.predict(X_test)\n",
    "    \n",
    "test_predictions = np.expm1(test_predictions_log)\n",
    "print(\"Predictions generated.\")\n",
    "\n",
    "# --- Create Submission File ---\n",
    "print(f\"\\nCreating submission file 'submission_best_model.csv'...\")\n",
    "submission = pd.DataFrame({\n",
    "    'Item_Identifier': original_test_identifiers['Item_Identifier'],\n",
    "    'Outlet_Identifier': original_test_identifiers['Outlet_Identifier'],\n",
    "    'Item_Outlet_Sales': test_predictions\n",
    "})\n",
    "\n",
    "# Save the submission file without the index.\n",
    "submission.to_csv('submission_best_model.csv', index=False)\n",
    "print(\"Submission file 'submission_best_model.csv' created successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6384b96",
   "metadata": {},
   "source": [
    "### 3. Improving Linear Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "cc9c7c88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading enhanced preprocessed training and test data...\n",
      "\n",
      "Starting K-Fold Cross-Validation and Hyperparameter Tuning for each model...\n",
      "\n",
      "Tuning Ridge model...\n",
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n",
      "-> Ridge completed. Best Parameters: {'alpha': 0.1}\n",
      "-> Best cross-validation RMSE: 0.5356\n",
      "\n",
      "Tuning Lasso model...\n",
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n",
      "-> Lasso completed. Best Parameters: {'alpha': 0.0001}\n",
      "-> Best cross-validation RMSE: 0.5356\n",
      "\n",
      "Tuning ElasticNet model...\n",
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n",
      "-> ElasticNet completed. Best Parameters: {'alpha': 0.0001}\n",
      "-> Best cross-validation RMSE: 0.5356\n",
      "\n",
      "Overall best model is Lasso with a cross-validation RMSE of 0.5356\n",
      "Training the best model on the full training data...\n",
      "Making predictions on the test data...\n",
      "Predictions generated.\n",
      "\n",
      "Creating submission file 'submission_linear_tuned.csv'...\n",
      "Submission file 'submission_linear_tuned.csv' created successfully!\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "from sklearn.model_selection import GridSearchCV, KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# --- Load Preprocessed Data ---\n",
    "# We are loading the enhanced preprocessed data.\n",
    "print(\"Loading enhanced preprocessed training and test data...\")\n",
    "train_df = pd.read_csv('preprocessed_train_v2.csv')\n",
    "test_df = pd.read_csv('preprocessed_test_v2.csv')\n",
    "\n",
    "# Also load the original test identifiers to build the submission file.\n",
    "original_test_identifiers = pd.read_csv('test_AbJTz2l.csv')[['Item_Identifier', 'Outlet_Identifier']]\n",
    "\n",
    "# --- Define Features and Target ---\n",
    "y_train_log = np.log1p(train_df['Item_Outlet_Sales'])\n",
    "X_train = train_df.drop('Item_Outlet_Sales', axis=1)\n",
    "X_test = test_df.copy()\n",
    "\n",
    "# --- Define Models to Compare ---\n",
    "# We will compare three different types of regularized linear models\n",
    "# and tune their hyperparameters to find the best fit.\n",
    "models_to_tune = {\n",
    "    'Ridge': Ridge(),\n",
    "    'Lasso': Lasso(),\n",
    "    'ElasticNet': ElasticNet()\n",
    "}\n",
    "\n",
    "# Define a common parameter grid for the regularization strength (alpha).\n",
    "# We'll test a wide range of values to find the optimal one for each model.\n",
    "param_grid = {\n",
    "    'alpha': [0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000]\n",
    "}\n",
    "\n",
    "# --- K-Fold Cross-Validation and Hyperparameter Tuning ---\n",
    "print(\"\\nStarting K-Fold Cross-Validation and Hyperparameter Tuning for each model...\")\n",
    "\n",
    "# Dictionary to store the best mean RMSE for each model.\n",
    "best_rmse_scores = {}\n",
    "best_estimators = {}\n",
    "\n",
    "for name, model in models_to_tune.items():\n",
    "    print(f\"\\nTuning {name} model...\")\n",
    "    \n",
    "    # Set up GridSearchCV for the current model.\n",
    "    grid_search = GridSearchCV(\n",
    "        estimator=model,\n",
    "        param_grid=param_grid,\n",
    "        scoring='neg_mean_squared_error',\n",
    "        cv=5, # Using 5-fold cross-validation\n",
    "        verbose=1,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    # Fit the grid search to the data to find the best alpha.\n",
    "    grid_search.fit(X_train, y_train_log)\n",
    "    \n",
    "    # Get the best estimator and its cross-validation score.\n",
    "    best_estimator = grid_search.best_estimator_\n",
    "    best_score = np.sqrt(abs(grid_search.best_score_))\n",
    "    \n",
    "    # Store the best estimator and its corresponding RMSE.\n",
    "    best_estimators[name] = best_estimator\n",
    "    best_rmse_scores[name] = best_score\n",
    "    \n",
    "    print(f\"-> {name} completed. Best Parameters: {grid_search.best_params_}\")\n",
    "    print(f\"-> Best cross-validation RMSE: {best_score:.4f}\")\n",
    "\n",
    "# --- Select the Overall Best Model and Make Predictions ---\n",
    "# Find the model with the lowest (best) mean RMSE from all tuned models.\n",
    "best_model_name = min(best_rmse_scores, key=best_rmse_scores.get)\n",
    "best_model = best_estimators[best_model_name]\n",
    "\n",
    "print(f\"\\nOverall best model is {best_model_name} with a cross-validation RMSE of {best_rmse_scores[best_model_name]:.4f}\")\n",
    "\n",
    "# Train the overall best model on the entire dataset one last time.\n",
    "print(\"Training the best model on the full training data...\")\n",
    "best_model.fit(X_train, y_train_log)\n",
    "\n",
    "# Make predictions on the test data.\n",
    "print(\"Making predictions on the test data...\")\n",
    "test_predictions_log = best_model.predict(X_test)\n",
    "\n",
    "# Inverse transform the predictions back to the original sales scale.\n",
    "test_predictions = np.expm1(test_predictions_log)\n",
    "print(\"Predictions generated.\")\n",
    "\n",
    "# --- Create Submission File ---\n",
    "print(f\"\\nCreating submission file 'submission_linear_tuned.csv'...\")\n",
    "submission = pd.DataFrame({\n",
    "    'Item_Identifier': original_test_identifiers['Item_Identifier'],\n",
    "    'Outlet_Identifier': original_test_identifiers['Outlet_Identifier'],\n",
    "    'Item_Outlet_Sales': test_predictions\n",
    "})\n",
    "\n",
    "# Save the submission file without the index.\n",
    "submission.to_csv('submission_linear_tuned.csv', index=False)\n",
    "print(\"Submission file 'submission_linear_tuned.csv' created successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f58e54c",
   "metadata": {},
   "source": [
    "### 4. Creating Preprocessed Train & Test (More Detailed + One hot + Normalised)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "fb68207d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading original raw training and test data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_15992\\2359433743.py:14: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  train_df['Item_Weight'].fillna(train_df.groupby('Item_Identifier')['Item_Weight'].transform('mean'), inplace=True)\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_15992\\2359433743.py:15: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  test_df['Item_Weight'].fillna(test_df.groupby('Item_Identifier')['Item_Weight'].transform('mean'), inplace=True)\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_15992\\2359433743.py:51: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  train_df['Item_Visibility'].replace(0, train_df['Item_Visibility'].mean(), inplace=True)\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_15992\\2359433743.py:52: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  test_df['Item_Visibility'].replace(0, test_df['Item_Visibility'].mean(), inplace=True)\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_15992\\2359433743.py:67: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  test_df_encoded['Item_Type_Encoded'].fillna(train_df_encoded['Item_Type_Encoded'].mean(), inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Normalizing numerical features...\n",
      "\n",
      "Saving new preprocessed datasets...\n",
      "\n",
      "Advanced preprocessing complete! Files 'preprocessed_train_advanced.csv', 'preprocessed_train_target.csv', and 'preprocessed_test_advanced.csv' have been created.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "import category_encoders as ce\n",
    "\n",
    "# --- Load Original Raw Data ---\n",
    "# We are starting from the original raw data files to apply a full range of preprocessing steps.\n",
    "print(\"Loading original raw training and test data...\")\n",
    "train_df = pd.read_csv('train_cleaned.csv')\n",
    "test_df = pd.read_csv('test_AbJTz2l.csv')\n",
    "\n",
    "# --- Data Cleaning and Feature Engineering (Common Steps) ---\n",
    "# These are necessary cleaning steps applied to both datasets.\n",
    "\n",
    "# Fill missing Item_Weight with the mean of the corresponding Item_Identifier\n",
    "train_df['Item_Weight'].fillna(train_df.groupby('Item_Identifier')['Item_Weight'].transform('mean'), inplace=True)\n",
    "test_df['Item_Weight'].fillna(test_df.groupby('Item_Identifier')['Item_Weight'].transform('mean'), inplace=True)\n",
    "\n",
    "# Replace 'Unknown' Outlet_Size with mode of corresponding Outlet_Type\n",
    "outlet_size_mode = train_df.groupby('Outlet_Type')['Outlet_Size'].apply(lambda x: x.mode()[0])\n",
    "for outlet_type, outlet_size in outlet_size_mode.items():\n",
    "    train_df.loc[(train_df['Outlet_Type'] == outlet_type) & (train_df['Outlet_Size'] == 'Unknown'), 'Outlet_Size'] = outlet_size\n",
    "    test_df.loc[(test_df['Outlet_Type'] == outlet_type) & (test_df['Outlet_Size'].isnull()), 'Outlet_Size'] = outlet_size\n",
    "\n",
    "# Fix inconsistent Item_Fat_Content\n",
    "# Correct the test file's Item_Fat_Content to match the train file's cleaned values.\n",
    "train_df['Item_Fat_Content'] = train_df['Item_Fat_Content'].replace(['low fat', 'reg'], ['Low Fat', 'Regular'])\n",
    "test_df['Item_Fat_Content'] = test_df['Item_Fat_Content'].replace(['low fat', 'reg', 'LF'], ['Low Fat', 'Regular', 'Low Fat'])\n",
    "\n",
    "# Create a new feature for Outlet_Years\n",
    "train_df['Outlet_Years'] = 2013 - train_df['Outlet_Establishment_Year']\n",
    "test_df['Outlet_Years'] = 2013 - test_df['Outlet_Establishment_Year']\n",
    "\n",
    "# Create a new feature for Item_MRP Category\n",
    "train_df['Item_MRP_Category'] = pd.cut(train_df['Item_MRP'], bins=4, labels=[1, 2, 3, 4])\n",
    "test_df['Item_MRP_Category'] = pd.cut(test_df['Item_MRP'], bins=4, labels=[1, 2, 3, 4])\n",
    "\n",
    "# Create a combined Item_Type feature for simplicity\n",
    "train_df['Item_Type_Combined'] = train_df['Item_Identifier'].apply(lambda x: x[0:2])\n",
    "test_df['Item_Type_Combined'] = test_df['Item_Identifier'].apply(lambda x: x[0:2])\n",
    "train_df['Item_Type_Combined'] = train_df['Item_Type_Combined'].map({'FD':'Food', 'NC':'Non-Consumable', 'DR':'Drinks'})\n",
    "test_df['Item_Type_Combined'] = test_df['Item_Type_Combined'].map({'FD':'Food', 'NC':'Non-Consumable', 'DR':'Drinks'})\n",
    "\n",
    "# --- Advanced Feature Engineering ---\n",
    "# These are new features that capture interactions and complex relationships.\n",
    "\n",
    "# Create an interaction feature between Item_Type and Outlet_Type\n",
    "train_df['Item_Outlet_Type'] = train_df['Item_Type'].astype(str) + '_' + train_df['Outlet_Type'].astype(str)\n",
    "test_df['Item_Outlet_Type'] = test_df['Item_Type'].astype(str) + '_' + test_df['Outlet_Type'].astype(str)\n",
    "\n",
    "# Create a new feature for Item_Visibility category\n",
    "# We'll handle the zero visibility values by replacing them with the mean before binning.\n",
    "train_df['Item_Visibility'].replace(0, train_df['Item_Visibility'].mean(), inplace=True)\n",
    "test_df['Item_Visibility'].replace(0, test_df['Item_Visibility'].mean(), inplace=True)\n",
    "train_df['Item_Visibility_Category'] = pd.qcut(train_df['Item_Visibility'], 4, labels=['Low', 'Medium', 'High', 'Very_High'])\n",
    "test_df['Item_Visibility_Category'] = pd.qcut(test_df['Item_Visibility'], 4, labels=['Low', 'Medium', 'High', 'Very_High'])\n",
    "\n",
    "# --- Apply Multiple Encoding Techniques ---\n",
    "\n",
    "# One-Hot Encoding for nominal features, applied to each dataset separately.\n",
    "train_df_encoded = pd.get_dummies(train_df, columns=['Outlet_Size', 'Outlet_Location_Type', 'Outlet_Type', 'Item_Fat_Content', 'Item_Outlet_Type', 'Item_Visibility_Category'], dtype=int)\n",
    "test_df_encoded = pd.get_dummies(test_df, columns=['Outlet_Size', 'Outlet_Location_Type', 'Outlet_Type', 'Item_Fat_Content', 'Item_Outlet_Type', 'Item_Visibility_Category'], dtype=int)\n",
    "\n",
    "# Target Encoding for a high-cardinality feature. We fit on the training data and transform both.\n",
    "encoder = ce.TargetEncoder(cols=['Item_Type'])\n",
    "train_df_encoded['Item_Type_Encoded'] = encoder.fit_transform(train_df_encoded['Item_Type'], train_df_encoded['Item_Outlet_Sales'])\n",
    "test_df_encoded['Item_Type_Encoded'] = encoder.transform(test_df_encoded['Item_Type'])\n",
    "# Fill any missing values created by the target encoder on the test set\n",
    "test_df_encoded['Item_Type_Encoded'].fillna(train_df_encoded['Item_Type_Encoded'].mean(), inplace=True)\n",
    "\n",
    "# Label Encoding for ordinal-like features\n",
    "le = LabelEncoder()\n",
    "train_df_encoded['Item_MRP_Category_LE'] = le.fit_transform(train_df_encoded['Item_MRP_Category'])\n",
    "test_df_encoded['Item_MRP_Category_LE'] = le.transform(test_df_encoded['Item_MRP_Category'])\n",
    "train_df_encoded['Item_Type_Combined_LE'] = le.fit_transform(train_df_encoded['Item_Type_Combined'])\n",
    "test_df_encoded['Item_Type_Combined_LE'] = le.transform(test_df_encoded['Item_Type_Combined'])\n",
    "\n",
    "# --- Normalize Numerical Features ---\n",
    "# We normalize key numerical features to a standard scale (0-1).\n",
    "# This is crucial for models that are sensitive to feature magnitude.\n",
    "print(\"\\nNormalizing numerical features...\")\n",
    "scaler = MinMaxScaler()\n",
    "numerical_cols = ['Item_Weight', 'Item_Visibility', 'Item_MRP']\n",
    "\n",
    "# Fit the scaler on the training data and transform both datasets.\n",
    "train_df_encoded[numerical_cols] = scaler.fit_transform(train_df_encoded[numerical_cols])\n",
    "test_df_encoded[numerical_cols] = scaler.transform(test_df_encoded[numerical_cols])\n",
    "\n",
    "\n",
    "# --- Select Final Features and Save Files ---\n",
    "# Define a list of final features to ensure consistency between train and test sets.\n",
    "base_features = [\n",
    "    'Item_Weight', 'Item_Visibility', 'Outlet_Years', 'Item_MRP',\n",
    "    'Item_Type_Encoded', 'Item_MRP_Category_LE', 'Item_Type_Combined_LE'\n",
    "]\n",
    "# Get all one-hot encoded columns from the training set.\n",
    "ohe_cols_train = [col for col in train_df_encoded.columns if any(x in col for x in ['Outlet_Size_', 'Outlet_Location_Type_', 'Outlet_Type_', 'Item_Fat_Content_', 'Item_Outlet_Type_', 'Item_Visibility_Category_'])]\n",
    "\n",
    "final_features = base_features + ohe_cols_train\n",
    "\n",
    "# Align columns to ensure both datasets have the same features.\n",
    "X_train_final = train_df_encoded[final_features]\n",
    "X_test_final = test_df_encoded.reindex(columns=final_features, fill_value=0)\n",
    "\n",
    "y_train_final = train_df_encoded['Item_Outlet_Sales']\n",
    "\n",
    "# Save the new preprocessed files\n",
    "print(\"\\nSaving new preprocessed datasets...\")\n",
    "X_train_final.to_csv('preprocessed_train_advanced.csv', index=False)\n",
    "y_train_final.to_csv('preprocessed_train_target.csv', index=False, header=True)\n",
    "X_test_final.to_csv('preprocessed_test_advanced.csv', index=False)\n",
    "\n",
    "print(\"\\nAdvanced preprocessing complete! Files 'preprocessed_train_advanced.csv', 'preprocessed_train_target.csv', and 'preprocessed_test_advanced.csv' have been created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12a225fe",
   "metadata": {},
   "source": [
    "### 5. Using Advanced Preprocessed Data for Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b841ccd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading advanced preprocessed training and test data...\n",
      "\n",
      "Checking for missing values...\n",
      "NaNs in X_train:\n",
      "Item_Weight                                         4\n",
      "Item_Visibility                                     0\n",
      "Outlet_Years                                        0\n",
      "Item_MRP                                            0\n",
      "Item_Type_Encoded                                   0\n",
      "                                                   ..\n",
      "Item_Outlet_Type_Starchy Foods_Supermarket Type3    0\n",
      "Item_Visibility_Category_Low                        0\n",
      "Item_Visibility_Category_Medium                     0\n",
      "Item_Visibility_Category_High                       0\n",
      "Item_Visibility_Category_Very_High                  0\n",
      "Length: 88, dtype: int64\n",
      "\n",
      "NaNs in X_test:\n",
      "Item_Weight                                         20\n",
      "Item_Visibility                                      0\n",
      "Outlet_Years                                         0\n",
      "Item_MRP                                             0\n",
      "Item_Type_Encoded                                    0\n",
      "                                                    ..\n",
      "Item_Outlet_Type_Starchy Foods_Supermarket Type3     0\n",
      "Item_Visibility_Category_Low                         0\n",
      "Item_Visibility_Category_Medium                      0\n",
      "Item_Visibility_Category_High                        0\n",
      "Item_Visibility_Category_Very_High                   0\n",
      "Length: 88, dtype: int64\n",
      "\n",
      "All missing values have been filled with 0.\n",
      "Splitting training data into training and validation sets...\n",
      "\n",
      "Initializing and training models...\n",
      "\n",
      "--- Model Performance Comparison ---\n",
      "\n",
      "Training Linear Regression...\n",
      "Linear Regression RMSE: 1072.64\n",
      "Linear Regression R-squared: 0.58\n",
      "\n",
      "Training Random Forest Regressor...\n",
      "Random Forest Regressor RMSE: 1088.19\n",
      "Random Forest Regressor R-squared: 0.56\n",
      "\n",
      "Training Gradient Boosting Regressor...\n",
      "Gradient Boosting Regressor RMSE: 1030.86\n",
      "Gradient Boosting Regressor R-squared: 0.61\n",
      "\n",
      "Finding optimal blending weight using GridSearchCV...\n",
      "Fitting 5 folds for each of 11 candidates, totalling 55 fits\n",
      "Optimal blending weight found: 0.50\n",
      "\n",
      "--- Final Prediction on Test Data ---\n",
      "Training the best model on the full training dataset...\n",
      "\n",
      "Correcting final predictions for submission format...\n",
      "Final predictions saved to 'final_predictions.csv' in the correct format!\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "\n",
    "# --- Load the Advanced Preprocessed Data ---\n",
    "# This code loads the training features, training target, and test features\n",
    "# that were generated by your previous preprocessing script, now with normalized columns.\n",
    "print(\"Loading advanced preprocessed training and test data...\")\n",
    "try:\n",
    "    X_train = pd.read_csv('preprocessed_train_advanced.csv')\n",
    "    y_train = pd.read_csv('preprocessed_train_target.csv').iloc[:, 0]\n",
    "    X_test = pd.read_csv('preprocessed_test_advanced.csv')\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Error: Required file not found. Please ensure that 'preprocessed_train_advanced.csv', 'preprocessed_train_target.csv', and 'preprocessed_test_advanced.csv' have been generated. Details: {e}\")\n",
    "    exit()\n",
    "\n",
    "# --- Check for and Handle NaN Values ---\n",
    "# It's crucial to ensure there are no missing values before training models.\n",
    "# Let's check the number of NaNs in each DataFrame.\n",
    "print(\"\\nChecking for missing values...\")\n",
    "print(\"NaNs in X_train:\")\n",
    "print(X_train.isnull().sum())\n",
    "print(\"\\nNaNs in X_test:\")\n",
    "print(X_test.isnull().sum())\n",
    "\n",
    "# Fill any remaining NaNs with a value (e.g., 0) to prevent model errors.\n",
    "# This is a critical step for models like Linear Regression and Gradient Boosting.\n",
    "X_train.fillna(0, inplace=True)\n",
    "X_test.fillna(0, inplace=True)\n",
    "print(\"\\nAll missing values have been filled with 0.\")\n",
    "\n",
    "# --- Split the Training Data ---\n",
    "# It's good practice to split the training data into a smaller training set\n",
    "# and a validation set to evaluate model performance on unseen data.\n",
    "print(\"Splitting training data into training and validation sets...\")\n",
    "X_train_split, X_val_split, y_train_split, y_val_split = train_test_split(\n",
    "    X_train, y_train, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# --- Initialize and Train the Models ---\n",
    "# We use a dictionary to hold our models for easy iteration and comparison.\n",
    "print(\"\\nInitializing and training models...\")\n",
    "models = {\n",
    "    'Linear Regression': LinearRegression(),\n",
    "    'Random Forest Regressor': RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1),\n",
    "    'Gradient Boosting Regressor': GradientBoostingRegressor(n_estimators=100, random_state=42)\n",
    "}\n",
    "\n",
    "# --- Compare Model Performance ---\n",
    "# Loop through each model, train it, make predictions, and evaluate its performance.\n",
    "print(\"\\n--- Model Performance Comparison ---\")\n",
    "for name, model in models.items():\n",
    "    print(f\"\\nTraining {name}...\")\n",
    "    try:\n",
    "        model.fit(X_train_split, y_train_split)\n",
    "        y_pred = model.predict(X_val_split)\n",
    "        \n",
    "        # Evaluate the model's performance using Root Mean Squared Error (RMSE) and R-squared.\n",
    "        rmse = np.sqrt(mean_squared_error(y_val_split, y_pred))\n",
    "        r2 = r2_score(y_val_split, y_pred)\n",
    "        \n",
    "        print(f\"{name} RMSE: {rmse:.2f}\")\n",
    "        print(f\"{name} R-squared: {r2:.2f}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during training or evaluation of {name}. Details: {e}\")\n",
    "\n",
    "# --- Add Blender Class and Grid Search for Blending ---\n",
    "# Define a custom scikit-learn compatible estimator for model blending.\n",
    "# This class needs to be defined within the script to be used with GridSearchCV.\n",
    "class Blender(BaseEstimator, RegressorMixin):\n",
    "    \"\"\"\n",
    "    A custom scikit-learn compatible regressor for blending predictions from two base models.\n",
    "    \"\"\"\n",
    "    def __init__(self, weight=0.5):\n",
    "        self.weight = weight\n",
    "        self.reg1 = LinearRegression()\n",
    "        # Using the same random state for consistency.\n",
    "        self.reg2 = RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Fits the two base regressors on the training data.\"\"\"\n",
    "        self.reg1.fit(X, y)\n",
    "        self.reg2.fit(X, y)\n",
    "        return self\n",
    "        \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Combines predictions from the two base models using a weighted average.\n",
    "        \n",
    "        Args:\n",
    "            X (pd.DataFrame): The feature data to make predictions on.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: The blended predictions.\n",
    "        \"\"\"\n",
    "        pred1 = self.reg1.predict(X)\n",
    "        pred2 = self.reg2.predict(X)\n",
    "        return self.weight * pred1 + (1 - self.weight) * pred2\n",
    "    \n",
    "    def get_params(self, deep=True):\n",
    "        \"\"\"Required method for scikit-learn compatibility.\"\"\"\n",
    "        return {'weight': self.weight}\n",
    "\n",
    "# Initialize the blending model and find the optimal weight using a grid search.\n",
    "blender = Blender()\n",
    "blending_param_grid = {'weight': np.arange(0.0, 1.01, 0.1)}\n",
    "\n",
    "print(\"\\nFinding optimal blending weight using GridSearchCV...\")\n",
    "blending_grid = GridSearchCV(\n",
    "    estimator=blender,\n",
    "    param_grid=blending_param_grid,\n",
    "    scoring='neg_mean_squared_error',\n",
    "    cv=5,\n",
    "    verbose=1,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "try:\n",
    "    # Fit the grid search on the training data.\n",
    "    blending_grid.fit(X_train_split, y_train_split)\n",
    "    best_weight = blending_grid.best_params_['weight']\n",
    "    print(f\"Optimal blending weight found: {best_weight:.2f}\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during GridSearchCV. Please check your data and class definition. Details: {e}\")\n",
    "    best_weight = 0.5 # Use a default weight if the grid search fails.\n",
    "    print(f\"Using default blending weight of {best_weight:.2f}.\")\n",
    "\n",
    "# --- Final Training and Prediction ---\n",
    "# We will use the best-performing model (or the newly optimized blender)\n",
    "# to make predictions on the final test set.\n",
    "print(\"\\n--- Final Prediction on Test Data ---\")\n",
    "\n",
    "# For demonstration, we'll use the best model found by the grid search.\n",
    "# This is a good practice as it has been validated on your training data.\n",
    "# The try/except block handles the AttributeError if GridSearchCV fails.\n",
    "try:\n",
    "    best_model = blending_grid.best_estimator_\n",
    "except AttributeError:\n",
    "    print(\"GridSearchCV failed to find a best estimator. Defaulting to a Gradient Boosting Regressor.\")\n",
    "    best_model = GradientBoostingRegressor(n_estimators=100, random_state=42)\n",
    "\n",
    "print(f\"Training the best model on the full training dataset...\")\n",
    "best_model.fit(X_train, y_train)\n",
    "\n",
    "# Make the final predictions\n",
    "test_predictions = best_model.predict(X_test)\n",
    "\n",
    "# --- Post-processing and Final Output ---\n",
    "# This section ensures the final output is in the correct format for submission.\n",
    "print(\"\\nCorrecting final predictions for submission format...\")\n",
    "\n",
    "# First, clip negative values to 0, as sales cannot be negative.\n",
    "test_predictions[test_predictions < 0] = 0\n",
    "\n",
    "# Next, load the original test file to get the identifiers.\n",
    "test_original_df = pd.read_csv('test_AbJTz2l.csv')\n",
    "\n",
    "# Create the final submission DataFrame.\n",
    "submission_df = pd.DataFrame({\n",
    "    'Item_Identifier': test_original_df['Item_Identifier'],\n",
    "    'Outlet_Identifier': test_original_df['Outlet_Identifier'],\n",
    "    'Item_Outlet_Sales': test_predictions\n",
    "})\n",
    "\n",
    "# Save the predictions to a CSV file.\n",
    "submission_df.to_csv('final_predictions.csv', index=False)\n",
    "print(\"Final predictions saved to 'final_predictions.csv' in the correct format!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ec56a9a",
   "metadata": {},
   "source": [
    "### 6. Creating Better Preprocessed Train & Test (More Detailed + One hot + Normalised)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a309e759",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading original raw training and test data...\n",
      "\n",
      "Log-transforming 'Item_Visibility' to correct for skewness...\n",
      "Applying Min-Max scaling to 'Item_Weight' and 'Item_MRP'...\n",
      "Applying one-hot encoding to categorical features...\n",
      "\n",
      "Saving new preprocessed datasets...\n",
      "\n",
      "Simplified preprocessing complete! Files 'preprocessed_train_simplified.csv', 'preprocessed_train_target_simplified.csv', and 'preprocessed_test_simplified.csv' have been created.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_15992\\3629662846.py:15: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  train_df['Item_Weight'].fillna(train_df.groupby('Item_Identifier')['Item_Weight'].transform('mean'), inplace=True)\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_15992\\3629662846.py:16: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  test_df['Item_Weight'].fillna(test_df.groupby('Item_Identifier')['Item_Weight'].transform('mean'), inplace=True)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# --- Load Original Raw Data ---\n",
    "# We are starting from the original raw data files to apply a full range of preprocessing steps.\n",
    "print(\"Loading original raw training and test data...\")\n",
    "train_df = pd.read_csv('train_cleaned.csv')\n",
    "test_df = pd.read_csv('test_AbJTz2l.csv')\n",
    "\n",
    "# --- Data Cleaning and Feature Engineering (Common Steps) ---\n",
    "# These are necessary cleaning steps applied to both datasets.\n",
    "\n",
    "# Fill missing Item_Weight with the mean of the corresponding Item_Identifier\n",
    "train_df['Item_Weight'].fillna(train_df.groupby('Item_Identifier')['Item_Weight'].transform('mean'), inplace=True)\n",
    "test_df['Item_Weight'].fillna(test_df.groupby('Item_Identifier')['Item_Weight'].transform('mean'), inplace=True)\n",
    "\n",
    "# Drop Outlet_Establishment_Year as it is not needed.\n",
    "train_df.drop('Outlet_Establishment_Year', axis=1, inplace=True)\n",
    "test_df.drop('Outlet_Establishment_Year', axis=1, inplace=True)\n",
    "\n",
    "# Fix inconsistent Item_Fat_Content values\n",
    "train_df['Item_Fat_Content'] = train_df['Item_Fat_Content'].replace(['low fat', 'reg'], ['Low Fat', 'Regular'])\n",
    "test_df['Item_Fat_Content'] = test_df['Item_Fat_Content'].replace(['low fat', 'reg', 'LF'], ['Low Fat', 'Regular', 'Low Fat'])\n",
    "\n",
    "# --- Normalize and Correct Skewness for Numerical Features ---\n",
    "print(\"\\nLog-transforming 'Item_Visibility' to correct for skewness...\")\n",
    "# We use np.log1p to handle any potential zero values gracefully.\n",
    "train_df['Item_Visibility'] = np.log1p(train_df['Item_Visibility'])\n",
    "test_df['Item_Visibility'] = np.log1p(test_df['Item_Visibility'])\n",
    "\n",
    "# Apply Min-Max scaling to the remaining numerical features: Item_Weight and Item_MRP.\n",
    "print(\"Applying Min-Max scaling to 'Item_Weight' and 'Item_MRP'...\")\n",
    "scaler = MinMaxScaler()\n",
    "numerical_cols = ['Item_Weight', 'Item_MRP']\n",
    "\n",
    "# Fit the scaler on the training data and transform both datasets.\n",
    "train_df[numerical_cols] = scaler.fit_transform(train_df[numerical_cols])\n",
    "test_df[numerical_cols] = scaler.transform(test_df[numerical_cols])\n",
    "\n",
    "# --- Apply One-Hot Encoding ---\n",
    "# One-Hot Encoding for all nominal features.\n",
    "print(\"Applying one-hot encoding to categorical features...\")\n",
    "one_hot_cols = ['Outlet_Size', 'Outlet_Location_Type', 'Outlet_Type', 'Item_Fat_Content', 'Item_Type']\n",
    "train_df_encoded = pd.get_dummies(train_df, columns=one_hot_cols, dtype=int)\n",
    "test_df_encoded = pd.get_dummies(test_df, columns=one_hot_cols, dtype=int)\n",
    "\n",
    "# Align columns to ensure both datasets have the same features after one-hot encoding.\n",
    "X_train_final, X_test_final = train_df_encoded.align(test_df_encoded, join='left', axis=1, fill_value=0)\n",
    "\n",
    "# --- Select Final Features and Save Files ---\n",
    "# Define the target variable for the training set.\n",
    "y_train_final = X_train_final['Item_Outlet_Sales']\n",
    "\n",
    "# Drop the columns from the final feature sets that are not needed for the model.\n",
    "columns_to_drop = ['Item_Outlet_Sales', 'Item_Identifier', 'Outlet_Identifier']\n",
    "X_train_final.drop(columns_to_drop, axis=1, errors='ignore', inplace=True)\n",
    "X_test_final.drop(columns_to_drop, axis=1, errors='ignore', inplace=True)\n",
    "\n",
    "# Save the new preprocessed files\n",
    "print(\"\\nSaving new preprocessed datasets...\")\n",
    "X_train_final.to_csv('preprocessed_train_simplified.csv', index=False)\n",
    "y_train_final.to_csv('preprocessed_train_target_simplified.csv', index=False, header=True)\n",
    "X_test_final.to_csv('preprocessed_test_simplified.csv', index=False)\n",
    "\n",
    "print(\"\\nSimplified preprocessing complete! Files 'preprocessed_train_simplified.csv', 'preprocessed_train_target_simplified.csv', and 'preprocessed_test_simplified.csv' have been created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a49c0c7",
   "metadata": {},
   "source": [
    "### 7. Model Fitting (Simplified)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6bf0bbf",
   "metadata": {},
   "source": [
    "#### 7.1. Comparing Linear Regression, Gradient Boost & Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7153db18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading advanced preprocessed training and test data...\n",
      "\n",
      "Checking for missing values...\n",
      "NaNs in X_train:\n",
      "Item_Weight                        4\n",
      "Item_Visibility                    0\n",
      "Item_MRP                           0\n",
      "Outlet_Size_High                   0\n",
      "Outlet_Size_Medium                 0\n",
      "Outlet_Size_Small                  0\n",
      "Outlet_Size_Unknown                0\n",
      "Outlet_Location_Type_Tier 1        0\n",
      "Outlet_Location_Type_Tier 2        0\n",
      "Outlet_Location_Type_Tier 3        0\n",
      "Outlet_Type_Grocery Store          0\n",
      "Outlet_Type_Supermarket Type1      0\n",
      "Outlet_Type_Supermarket Type2      0\n",
      "Outlet_Type_Supermarket Type3      0\n",
      "Item_Fat_Content_Low Fat           0\n",
      "Item_Fat_Content_Regular           0\n",
      "Item_Type_Baking Goods             0\n",
      "Item_Type_Breads                   0\n",
      "Item_Type_Breakfast                0\n",
      "Item_Type_Canned                   0\n",
      "Item_Type_Dairy                    0\n",
      "Item_Type_Frozen Foods             0\n",
      "Item_Type_Fruits and Vegetables    0\n",
      "Item_Type_Hard Drinks              0\n",
      "Item_Type_Health and Hygiene       0\n",
      "Item_Type_Household                0\n",
      "Item_Type_Meat                     0\n",
      "Item_Type_Others                   0\n",
      "Item_Type_Seafood                  0\n",
      "Item_Type_Snack Foods              0\n",
      "Item_Type_Soft Drinks              0\n",
      "Item_Type_Starchy Foods            0\n",
      "dtype: int64\n",
      "\n",
      "NaNs in X_test:\n",
      "Item_Weight                        20\n",
      "Item_Visibility                     0\n",
      "Item_MRP                            0\n",
      "Outlet_Size_High                    0\n",
      "Outlet_Size_Medium                  0\n",
      "Outlet_Size_Small                   0\n",
      "Outlet_Size_Unknown                 0\n",
      "Outlet_Location_Type_Tier 1         0\n",
      "Outlet_Location_Type_Tier 2         0\n",
      "Outlet_Location_Type_Tier 3         0\n",
      "Outlet_Type_Grocery Store           0\n",
      "Outlet_Type_Supermarket Type1       0\n",
      "Outlet_Type_Supermarket Type2       0\n",
      "Outlet_Type_Supermarket Type3       0\n",
      "Item_Fat_Content_Low Fat            0\n",
      "Item_Fat_Content_Regular            0\n",
      "Item_Type_Baking Goods              0\n",
      "Item_Type_Breads                    0\n",
      "Item_Type_Breakfast                 0\n",
      "Item_Type_Canned                    0\n",
      "Item_Type_Dairy                     0\n",
      "Item_Type_Frozen Foods              0\n",
      "Item_Type_Fruits and Vegetables     0\n",
      "Item_Type_Hard Drinks               0\n",
      "Item_Type_Health and Hygiene        0\n",
      "Item_Type_Household                 0\n",
      "Item_Type_Meat                      0\n",
      "Item_Type_Others                    0\n",
      "Item_Type_Seafood                   0\n",
      "Item_Type_Snack Foods               0\n",
      "Item_Type_Soft Drinks               0\n",
      "Item_Type_Starchy Foods             0\n",
      "dtype: int64\n",
      "\n",
      "All missing values have been filled with 0.\n",
      "Splitting training data into training and validation sets...\n",
      "\n",
      "Initializing and training models...\n",
      "\n",
      "--- Model Performance Comparison ---\n",
      "\n",
      "Training Linear Regression...\n",
      "Linear Regression RMSE: 1071.23\n",
      "Linear Regression R-squared: 0.58\n",
      "\n",
      "Training Random Forest Regressor...\n",
      "Random Forest Regressor RMSE: 1093.89\n",
      "Random Forest Regressor R-squared: 0.56\n",
      "\n",
      "Training Gradient Boosting Regressor...\n",
      "Gradient Boosting Regressor RMSE: 1038.29\n",
      "Gradient Boosting Regressor R-squared: 0.60\n",
      "\n",
      "Finding optimal blending weight using GridSearchCV...\n",
      "Fitting 5 folds for each of 11 candidates, totalling 55 fits\n",
      "Optimal blending weight found: 0.50\n",
      "\n",
      "--- Final Prediction on Test Data ---\n",
      "Training the best model on the full training dataset...\n",
      "\n",
      "Correcting final predictions for submission format...\n",
      "Final predictions saved to 'final_predictions_simplified.csv' in the correct format!\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "\n",
    "# --- Load the Advanced Preprocessed Data ---\n",
    "# This code loads the training features, training target, and test features\n",
    "# that were generated by your previous preprocessing script, now with normalized columns.\n",
    "print(\"Loading advanced preprocessed training and test data...\")\n",
    "try:\n",
    "    X_train = pd.read_csv('preprocessed_train_simplified.csv')\n",
    "    y_train = pd.read_csv('preprocessed_train_target_simplified.csv').iloc[:, 0]\n",
    "    X_test = pd.read_csv('preprocessed_test_simplified.csv')\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Error: Required file not found. Please ensure that 'preprocessed_train_advanced.csv', 'preprocessed_train_target.csv', and 'preprocessed_test_advanced.csv' have been generated. Details: {e}\")\n",
    "    exit()\n",
    "\n",
    "# --- Check for and Handle NaN Values ---\n",
    "# It's crucial to ensure there are no missing values before training models.\n",
    "# Let's check the number of NaNs in each DataFrame.\n",
    "print(\"\\nChecking for missing values...\")\n",
    "print(\"NaNs in X_train:\")\n",
    "print(X_train.isnull().sum())\n",
    "print(\"\\nNaNs in X_test:\")\n",
    "print(X_test.isnull().sum())\n",
    "\n",
    "# Fill any remaining NaNs with a value (e.g., 0) to prevent model errors.\n",
    "# This is a critical step for models like Linear Regression and Gradient Boosting.\n",
    "X_train.fillna(0, inplace=True)\n",
    "X_test.fillna(0, inplace=True)\n",
    "print(\"\\nAll missing values have been filled with 0.\")\n",
    "\n",
    "# --- Split the Training Data ---\n",
    "# It's good practice to split the training data into a smaller training set\n",
    "# and a validation set to evaluate model performance on unseen data.\n",
    "print(\"Splitting training data into training and validation sets...\")\n",
    "X_train_split, X_val_split, y_train_split, y_val_split = train_test_split(\n",
    "    X_train, y_train, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# --- Initialize and Train the Models ---\n",
    "# We use a dictionary to hold our models for easy iteration and comparison.\n",
    "print(\"\\nInitializing and training models...\")\n",
    "models = {\n",
    "    'Linear Regression': LinearRegression(),\n",
    "    'Random Forest Regressor': RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1),\n",
    "    'Gradient Boosting Regressor': GradientBoostingRegressor(n_estimators=100, random_state=42)\n",
    "}\n",
    "\n",
    "# --- Compare Model Performance ---\n",
    "# Loop through each model, train it, make predictions, and evaluate its performance.\n",
    "print(\"\\n--- Model Performance Comparison ---\")\n",
    "for name, model in models.items():\n",
    "    print(f\"\\nTraining {name}...\")\n",
    "    try:\n",
    "        model.fit(X_train_split, y_train_split)\n",
    "        y_pred = model.predict(X_val_split)\n",
    "        \n",
    "        # Evaluate the model's performance using Root Mean Squared Error (RMSE) and R-squared.\n",
    "        rmse = np.sqrt(mean_squared_error(y_val_split, y_pred))\n",
    "        r2 = r2_score(y_val_split, y_pred)\n",
    "        \n",
    "        print(f\"{name} RMSE: {rmse:.2f}\")\n",
    "        print(f\"{name} R-squared: {r2:.2f}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during training or evaluation of {name}. Details: {e}\")\n",
    "\n",
    "# --- Add Blender Class and Grid Search for Blending ---\n",
    "# Define a custom scikit-learn compatible estimator for model blending.\n",
    "# This class needs to be defined within the script to be used with GridSearchCV.\n",
    "class Blender(BaseEstimator, RegressorMixin):\n",
    "    \"\"\"\n",
    "    A custom scikit-learn compatible regressor for blending predictions from two base models.\n",
    "    \"\"\"\n",
    "    def __init__(self, weight=0.5):\n",
    "        self.weight = weight\n",
    "        self.reg1 = LinearRegression()\n",
    "        # Using the same random state for consistency.\n",
    "        self.reg2 = RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Fits the two base regressors on the training data.\"\"\"\n",
    "        self.reg1.fit(X, y)\n",
    "        self.reg2.fit(X, y)\n",
    "        return self\n",
    "        \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Combines predictions from the two base models using a weighted average.\n",
    "        \n",
    "        Args:\n",
    "            X (pd.DataFrame): The feature data to make predictions on.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: The blended predictions.\n",
    "        \"\"\"\n",
    "        pred1 = self.reg1.predict(X)\n",
    "        pred2 = self.reg2.predict(X)\n",
    "        return self.weight * pred1 + (1 - self.weight) * pred2\n",
    "    \n",
    "    def get_params(self, deep=True):\n",
    "        \"\"\"Required method for scikit-learn compatibility.\"\"\"\n",
    "        return {'weight': self.weight}\n",
    "\n",
    "# Initialize the blending model and find the optimal weight using a grid search.\n",
    "blender = Blender()\n",
    "blending_param_grid = {'weight': np.arange(0.0, 1.01, 0.1)}\n",
    "\n",
    "print(\"\\nFinding optimal blending weight using GridSearchCV...\")\n",
    "blending_grid = GridSearchCV(\n",
    "    estimator=blender,\n",
    "    param_grid=blending_param_grid,\n",
    "    scoring='neg_mean_squared_error',\n",
    "    cv=5,\n",
    "    verbose=1,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "try:\n",
    "    # Fit the grid search on the training data.\n",
    "    blending_grid.fit(X_train_split, y_train_split)\n",
    "    best_weight = blending_grid.best_params_['weight']\n",
    "    print(f\"Optimal blending weight found: {best_weight:.2f}\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during GridSearchCV. Please check your data and class definition. Details: {e}\")\n",
    "    best_weight = 0.5 # Use a default weight if the grid search fails.\n",
    "    print(f\"Using default blending weight of {best_weight:.2f}.\")\n",
    "\n",
    "# --- Final Training and Prediction ---\n",
    "# We will use the best-performing model (or the newly optimized blender)\n",
    "# to make predictions on the final test set.\n",
    "print(\"\\n--- Final Prediction on Test Data ---\")\n",
    "\n",
    "# For demonstration, we'll use the best model found by the grid search.\n",
    "# This is a good practice as it has been validated on your training data.\n",
    "# The try/except block handles the AttributeError if GridSearchCV fails.\n",
    "try:\n",
    "    best_model = blending_grid.best_estimator_\n",
    "except AttributeError:\n",
    "    print(\"GridSearchCV failed to find a best estimator. Defaulting to a Gradient Boosting Regressor.\")\n",
    "    best_model = GradientBoostingRegressor(n_estimators=100, random_state=42)\n",
    "\n",
    "print(f\"Training the best model on the full training dataset...\")\n",
    "best_model.fit(X_train, y_train)\n",
    "\n",
    "# Make the final predictions\n",
    "test_predictions = best_model.predict(X_test)\n",
    "\n",
    "# --- Post-processing and Final Output ---\n",
    "# This section ensures the final output is in the correct format for submission.\n",
    "print(\"\\nCorrecting final predictions for submission format...\")\n",
    "\n",
    "# First, clip negative values to 0, as sales cannot be negative.\n",
    "test_predictions[test_predictions < 0] = 0\n",
    "\n",
    "# Next, load the original test file to get the identifiers.\n",
    "test_original_df = pd.read_csv('test_AbJTz2l.csv')\n",
    "\n",
    "# Create the final submission DataFrame.\n",
    "submission_df = pd.DataFrame({\n",
    "    'Item_Identifier': test_original_df['Item_Identifier'],\n",
    "    'Outlet_Identifier': test_original_df['Outlet_Identifier'],\n",
    "    'Item_Outlet_Sales': test_predictions\n",
    "})\n",
    "\n",
    "# Save the predictions to a CSV file.\n",
    "submission_df.to_csv('final_predictions_simplified.csv', index=False)\n",
    "print(\"Final predictions saved to 'final_predictions_simplified.csv' in the correct format!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94b58619",
   "metadata": {},
   "source": [
    "### 8. Final Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b4abbe2",
   "metadata": {},
   "source": [
    "#### 8.1. Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d268b58e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading preprocessed training and test data from our shared files...\n",
      "\n",
      "Log-transforming the target variable 'Item_Outlet_Sales'...\n",
      "\n",
      "Starting hyperparameter tuning using GridSearchCV with RandomForestRegressor...\n",
      "Fitting 3 folds for each of 27 candidates, totalling 81 fits\n",
      "\n",
      "Best parameters found:  {'max_depth': 10, 'min_samples_leaf': 4, 'n_estimators': 500}\n",
      "Best cross-validation score (negative RMSE): -0.28\n",
      "\n",
      "Making predictions on the test data with the best model...\n",
      "Predictions generated.\n",
      "\n",
      "Creating new submission file 'submission_tuned_final.csv'...\n",
      "Submission file 'submission_tuned_final.csv' created successfully!\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# --- Load the Preprocessed Data ---\n",
    "# Loading the training features, training target, and test features from the\n",
    "# files we generated in previous steps.\n",
    "print(\"Loading preprocessed training and test data from our shared files...\")\n",
    "try:\n",
    "    # X_train contains the features\n",
    "    X_train = pd.read_csv('preprocessed_train_simplified.csv')\n",
    "    # y_train contains the target variable 'Item_Outlet_Sales'\n",
    "    y_train = pd.read_csv('preprocessed_train_target_simplified.csv').iloc[:, 0]\n",
    "    # X_test contains the features for the test set\n",
    "    X_test = pd.read_csv('preprocessed_test_simplified.csv')\n",
    "    # We also need the original test file to get the identifiers for the submission\n",
    "    original_test_identifiers = pd.read_csv('test_AbJTz2l.csv')[['Item_Identifier', 'Outlet_Identifier']]\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Error: Required file not found. Please ensure that 'preprocessed_train_simplified.csv', 'preprocessed_train_target_simplified.csv', 'preprocessed_test_simplified.csv', and 'test_AbJTz2l.csv' are in the directory. Details: {e}\")\n",
    "    exit()\n",
    "\n",
    "# --- Log-Transform the Target Variable ---\n",
    "# It is a common practice to log-transform skewed target variables to\n",
    "# improve the performance of regression models. We add 1 before taking the log\n",
    "# to handle any zero values.\n",
    "print(\"\\nLog-transforming the target variable 'Item_Outlet_Sales'...\")\n",
    "y_train_log = np.log1p(y_train)\n",
    "\n",
    "# --- Hyperparameter Tuning with GridSearchCV ---\n",
    "print(\"\\nStarting hyperparameter tuning using GridSearchCV with RandomForestRegressor...\")\n",
    "\n",
    "# Initialize the RandomForest Regressor model.\n",
    "# A random forest is an ensemble of decision trees.\n",
    "rf_reg = RandomForestRegressor(random_state=42, n_jobs=-1)\n",
    "\n",
    "# Define a parameter grid for the RandomForestRegressor.\n",
    "# We will tune key parameters that control model complexity and prevent overfitting.\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 500],  # Number of trees in the forest\n",
    "    'max_depth': [10, 20, None],      # Maximum depth of the trees\n",
    "    'min_samples_leaf': [1, 2, 4],    # Minimum number of samples required to be at a leaf node\n",
    "}\n",
    "\n",
    "# Set up GridSearchCV.\n",
    "# 'scoring'='neg_mean_squared_error' is used to find the model with the lowest MSE.\n",
    "# 'cv=3' means we will perform 3-fold cross-validation.\n",
    "# 'n_jobs=-1' uses all available CPU cores to speed up the process.\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=rf_reg,\n",
    "    param_grid=param_grid,\n",
    "    scoring='neg_mean_squared_error',\n",
    "    cv=3,\n",
    "    verbose=2,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Fit the grid search to the data. This will run the cross-validation\n",
    "# for every combination of parameters in the grid.\n",
    "grid_search.fit(X_train, y_train_log)\n",
    "\n",
    "# Print the best parameters and the best score found.\n",
    "print(\"\\nBest parameters found: \", grid_search.best_params_)\n",
    "print(\"Best cross-validation score (negative RMSE): {:.2f}\".format(grid_search.best_score_))\n",
    "\n",
    "# --- Make Predictions with the Best Model ---\n",
    "print(\"\\nMaking predictions on the test data with the best model...\")\n",
    "\n",
    "# The best model is stored in the `best_estimator_` attribute of the grid search object.\n",
    "best_rf_model = grid_search.best_estimator_\n",
    "test_predictions_log = best_rf_model.predict(X_test)\n",
    "\n",
    "# Inverse transform the predictions from the log scale back to the original scale.\n",
    "# np.expm1 is the inverse function of np.log1p.\n",
    "test_predictions = np.expm1(test_predictions_log)\n",
    "print(\"Predictions generated.\")\n",
    "\n",
    "# --- Create Submission File ---\n",
    "# The submission file requires the original item and outlet identifiers.\n",
    "print(\"\\nCreating new submission file 'submission_tuned_final.csv'...\")\n",
    "submission = pd.DataFrame({\n",
    "    'Item_Identifier': original_test_identifiers['Item_Identifier'],\n",
    "    'Outlet_Identifier': original_test_identifiers['Outlet_Identifier'],\n",
    "    'Item_Outlet_Sales': test_predictions\n",
    "})\n",
    "\n",
    "# Save the submission file to a CSV without the index column.\n",
    "submission.to_csv('submission_tuned_final.csv', index=False)\n",
    "print(\"Submission file 'submission_tuned_final.csv' created successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e1049a1",
   "metadata": {},
   "source": [
    "#### 8.2. Gradient Boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "11028ccb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading preprocessed training and test data...\n",
      "Missing values in dataframes filled with 0.\n",
      "\n",
      "Log-transforming the target variable 'Item_Outlet_Sales'...\n",
      "\n",
      "Starting hyperparameter tuning with GridSearchCV for GradientBoostingRegressor...\n",
      "Fitting 3 folds for each of 81 candidates, totalling 243 fits\n",
      "[CV] END learning_rate=0.05, max_depth=3, n_estimators=100, subsample=0.7; total time=   0.8s\n",
      "[CV] END learning_rate=0.05, max_depth=3, n_estimators=100, subsample=0.7; total time=   0.9s\n",
      "[CV] END learning_rate=0.05, max_depth=3, n_estimators=100, subsample=0.7; total time=   0.8s\n",
      "[CV] END learning_rate=0.05, max_depth=3, n_estimators=100, subsample=0.8; total time=   0.9s\n",
      "[CV] END learning_rate=0.05, max_depth=3, n_estimators=100, subsample=0.8; total time=   0.9s\n",
      "[CV] END learning_rate=0.05, max_depth=3, n_estimators=100, subsample=0.8; total time=   0.9s\n",
      "[CV] END learning_rate=0.05, max_depth=3, n_estimators=100, subsample=0.9; total time=   1.0s\n",
      "[CV] END learning_rate=0.05, max_depth=3, n_estimators=100, subsample=0.9; total time=   1.0s\n",
      "[CV] END learning_rate=0.05, max_depth=3, n_estimators=100, subsample=0.9; total time=   1.1s\n",
      "[CV] END learning_rate=0.05, max_depth=3, n_estimators=200, subsample=0.7; total time=   1.7s\n",
      "[CV] END learning_rate=0.05, max_depth=3, n_estimators=200, subsample=0.7; total time=   1.7s\n",
      "[CV] END learning_rate=0.05, max_depth=3, n_estimators=200, subsample=0.7; total time=   1.7s\n",
      "[CV] END learning_rate=0.05, max_depth=3, n_estimators=200, subsample=0.8; total time=   1.9s\n",
      "[CV] END learning_rate=0.05, max_depth=3, n_estimators=200, subsample=0.8; total time=   1.9s\n",
      "[CV] END learning_rate=0.05, max_depth=3, n_estimators=200, subsample=0.8; total time=   1.9s\n",
      "[CV] END learning_rate=0.05, max_depth=3, n_estimators=200, subsample=0.9; total time=   2.1s\n",
      "[CV] END learning_rate=0.05, max_depth=3, n_estimators=200, subsample=0.9; total time=   2.4s\n",
      "[CV] END learning_rate=0.05, max_depth=3, n_estimators=200, subsample=0.9; total time=   2.9s\n",
      "[CV] END learning_rate=0.05, max_depth=3, n_estimators=500, subsample=0.7; total time=   5.6s\n",
      "[CV] END learning_rate=0.05, max_depth=3, n_estimators=500, subsample=0.7; total time=   5.2s\n",
      "[CV] END learning_rate=0.05, max_depth=3, n_estimators=500, subsample=0.7; total time=   5.0s\n",
      "[CV] END learning_rate=0.05, max_depth=3, n_estimators=500, subsample=0.8; total time=   6.0s\n",
      "[CV] END learning_rate=0.05, max_depth=3, n_estimators=500, subsample=0.8; total time=   5.5s\n",
      "[CV] END learning_rate=0.05, max_depth=3, n_estimators=500, subsample=0.8; total time=   5.7s\n",
      "[CV] END learning_rate=0.05, max_depth=3, n_estimators=500, subsample=0.9; total time=   5.8s\n",
      "[CV] END learning_rate=0.05, max_depth=3, n_estimators=500, subsample=0.9; total time=   6.0s\n",
      "[CV] END learning_rate=0.05, max_depth=3, n_estimators=500, subsample=0.9; total time=   8.1s\n",
      "[CV] END learning_rate=0.05, max_depth=4, n_estimators=100, subsample=0.7; total time=   2.3s\n",
      "[CV] END learning_rate=0.05, max_depth=4, n_estimators=100, subsample=0.7; total time=   2.0s\n",
      "[CV] END learning_rate=0.05, max_depth=4, n_estimators=100, subsample=0.7; total time=   2.1s\n",
      "[CV] END learning_rate=0.05, max_depth=4, n_estimators=100, subsample=0.8; total time=   2.4s\n",
      "[CV] END learning_rate=0.05, max_depth=4, n_estimators=100, subsample=0.8; total time=   2.3s\n",
      "[CV] END learning_rate=0.05, max_depth=4, n_estimators=100, subsample=0.8; total time=   2.5s\n",
      "[CV] END learning_rate=0.05, max_depth=4, n_estimators=100, subsample=0.9; total time=   2.7s\n",
      "[CV] END learning_rate=0.05, max_depth=4, n_estimators=100, subsample=0.9; total time=   2.7s\n",
      "[CV] END learning_rate=0.05, max_depth=4, n_estimators=100, subsample=0.9; total time=   2.7s\n",
      "[CV] END learning_rate=0.05, max_depth=4, n_estimators=200, subsample=0.7; total time=   4.5s\n",
      "[CV] END learning_rate=0.05, max_depth=4, n_estimators=200, subsample=0.7; total time=   4.4s\n",
      "[CV] END learning_rate=0.05, max_depth=4, n_estimators=200, subsample=0.7; total time=   4.5s\n",
      "[CV] END learning_rate=0.05, max_depth=4, n_estimators=200, subsample=0.8; total time=   4.9s\n",
      "[CV] END learning_rate=0.05, max_depth=4, n_estimators=200, subsample=0.8; total time=   4.9s\n",
      "[CV] END learning_rate=0.05, max_depth=4, n_estimators=200, subsample=0.8; total time=   5.1s\n",
      "[CV] END learning_rate=0.05, max_depth=4, n_estimators=200, subsample=0.9; total time=   5.6s\n",
      "[CV] END learning_rate=0.05, max_depth=4, n_estimators=200, subsample=0.9; total time=   5.6s\n",
      "[CV] END learning_rate=0.05, max_depth=4, n_estimators=200, subsample=0.9; total time=   5.5s\n",
      "[CV] END learning_rate=0.05, max_depth=4, n_estimators=500, subsample=0.7; total time=  11.2s\n",
      "[CV] END learning_rate=0.05, max_depth=4, n_estimators=500, subsample=0.7; total time=  11.0s\n",
      "[CV] END learning_rate=0.05, max_depth=4, n_estimators=500, subsample=0.7; total time=  11.2s\n",
      "[CV] END learning_rate=0.05, max_depth=4, n_estimators=500, subsample=0.8; total time=  12.4s\n",
      "[CV] END learning_rate=0.05, max_depth=4, n_estimators=500, subsample=0.8; total time=  13.1s\n",
      "[CV] END learning_rate=0.05, max_depth=4, n_estimators=500, subsample=0.8; total time=  13.2s\n",
      "[CV] END learning_rate=0.05, max_depth=4, n_estimators=500, subsample=0.9; total time=  14.5s\n",
      "[CV] END learning_rate=0.05, max_depth=4, n_estimators=500, subsample=0.9; total time=  14.5s\n",
      "[CV] END learning_rate=0.05, max_depth=4, n_estimators=500, subsample=0.9; total time=  13.8s\n",
      "[CV] END learning_rate=0.05, max_depth=5, n_estimators=100, subsample=0.7; total time=   2.6s\n",
      "[CV] END learning_rate=0.05, max_depth=5, n_estimators=100, subsample=0.7; total time=   2.6s\n",
      "[CV] END learning_rate=0.05, max_depth=5, n_estimators=100, subsample=0.7; total time=   2.5s\n",
      "[CV] END learning_rate=0.05, max_depth=5, n_estimators=100, subsample=0.8; total time=   3.0s\n",
      "[CV] END learning_rate=0.05, max_depth=5, n_estimators=100, subsample=0.8; total time=   2.9s\n",
      "[CV] END learning_rate=0.05, max_depth=5, n_estimators=100, subsample=0.8; total time=   3.0s\n",
      "[CV] END learning_rate=0.05, max_depth=5, n_estimators=100, subsample=0.9; total time=   3.3s\n",
      "[CV] END learning_rate=0.05, max_depth=5, n_estimators=100, subsample=0.9; total time=   3.2s\n",
      "[CV] END learning_rate=0.05, max_depth=5, n_estimators=100, subsample=0.9; total time=   3.2s\n",
      "[CV] END learning_rate=0.05, max_depth=5, n_estimators=200, subsample=0.7; total time=   5.3s\n",
      "[CV] END learning_rate=0.05, max_depth=5, n_estimators=200, subsample=0.7; total time=   5.2s\n",
      "[CV] END learning_rate=0.05, max_depth=5, n_estimators=200, subsample=0.7; total time=   5.6s\n",
      "[CV] END learning_rate=0.05, max_depth=5, n_estimators=200, subsample=0.8; total time=   6.2s\n",
      "[CV] END learning_rate=0.05, max_depth=5, n_estimators=200, subsample=0.8; total time=   6.0s\n",
      "[CV] END learning_rate=0.05, max_depth=5, n_estimators=200, subsample=0.8; total time=   6.1s\n",
      "[CV] END learning_rate=0.05, max_depth=5, n_estimators=200, subsample=0.9; total time=   7.5s\n",
      "[CV] END learning_rate=0.05, max_depth=5, n_estimators=200, subsample=0.9; total time=   8.4s\n",
      "[CV] END learning_rate=0.05, max_depth=5, n_estimators=200, subsample=0.9; total time=   7.6s\n",
      "[CV] END learning_rate=0.05, max_depth=5, n_estimators=500, subsample=0.7; total time=  14.0s\n",
      "[CV] END learning_rate=0.05, max_depth=5, n_estimators=500, subsample=0.7; total time=  13.7s\n",
      "[CV] END learning_rate=0.05, max_depth=5, n_estimators=500, subsample=0.7; total time=  13.6s\n",
      "[CV] END learning_rate=0.05, max_depth=5, n_estimators=500, subsample=0.8; total time=  14.7s\n",
      "[CV] END learning_rate=0.05, max_depth=5, n_estimators=500, subsample=0.8; total time=  16.2s\n",
      "[CV] END learning_rate=0.05, max_depth=5, n_estimators=500, subsample=0.8; total time=  15.0s\n",
      "[CV] END learning_rate=0.05, max_depth=5, n_estimators=500, subsample=0.9; total time=  17.2s\n",
      "[CV] END learning_rate=0.05, max_depth=5, n_estimators=500, subsample=0.9; total time=  17.0s\n",
      "[CV] END learning_rate=0.05, max_depth=5, n_estimators=500, subsample=0.9; total time=  17.2s\n",
      "[CV] END learning_rate=0.1, max_depth=3, n_estimators=100, subsample=0.7; total time=   1.7s\n",
      "[CV] END learning_rate=0.1, max_depth=3, n_estimators=100, subsample=0.7; total time=   1.7s\n",
      "[CV] END learning_rate=0.1, max_depth=3, n_estimators=100, subsample=0.7; total time=   1.7s\n",
      "[CV] END learning_rate=0.1, max_depth=3, n_estimators=100, subsample=0.8; total time=   2.0s\n",
      "[CV] END learning_rate=0.1, max_depth=3, n_estimators=100, subsample=0.8; total time=   1.8s\n",
      "[CV] END learning_rate=0.1, max_depth=3, n_estimators=100, subsample=0.8; total time=   1.9s\n",
      "[CV] END learning_rate=0.1, max_depth=3, n_estimators=100, subsample=0.9; total time=   2.1s\n",
      "[CV] END learning_rate=0.1, max_depth=3, n_estimators=100, subsample=0.9; total time=   2.1s\n",
      "[CV] END learning_rate=0.1, max_depth=3, n_estimators=100, subsample=0.9; total time=   2.2s\n",
      "[CV] END learning_rate=0.1, max_depth=3, n_estimators=200, subsample=0.7; total time=   3.5s\n",
      "[CV] END learning_rate=0.1, max_depth=3, n_estimators=200, subsample=0.7; total time=   3.6s\n",
      "[CV] END learning_rate=0.1, max_depth=3, n_estimators=200, subsample=0.7; total time=   3.7s\n",
      "[CV] END learning_rate=0.1, max_depth=3, n_estimators=200, subsample=0.8; total time=   4.6s\n",
      "[CV] END learning_rate=0.1, max_depth=3, n_estimators=200, subsample=0.8; total time=   3.9s\n",
      "[CV] END learning_rate=0.1, max_depth=3, n_estimators=200, subsample=0.8; total time=   3.8s\n",
      "[CV] END learning_rate=0.1, max_depth=3, n_estimators=200, subsample=0.9; total time=   4.4s\n",
      "[CV] END learning_rate=0.1, max_depth=3, n_estimators=200, subsample=0.9; total time=   4.1s\n",
      "[CV] END learning_rate=0.1, max_depth=3, n_estimators=200, subsample=0.9; total time=   4.5s\n",
      "[CV] END learning_rate=0.1, max_depth=3, n_estimators=500, subsample=0.7; total time=   8.8s\n",
      "[CV] END learning_rate=0.1, max_depth=3, n_estimators=500, subsample=0.7; total time=   9.0s\n",
      "[CV] END learning_rate=0.1, max_depth=3, n_estimators=500, subsample=0.7; total time=   8.6s\n",
      "[CV] END learning_rate=0.1, max_depth=3, n_estimators=500, subsample=0.8; total time=   9.3s\n",
      "[CV] END learning_rate=0.1, max_depth=3, n_estimators=500, subsample=0.8; total time=  10.0s\n",
      "[CV] END learning_rate=0.1, max_depth=3, n_estimators=500, subsample=0.8; total time=   9.9s\n",
      "[CV] END learning_rate=0.1, max_depth=3, n_estimators=500, subsample=0.9; total time=  10.9s\n",
      "[CV] END learning_rate=0.1, max_depth=3, n_estimators=500, subsample=0.9; total time=  11.3s\n",
      "[CV] END learning_rate=0.1, max_depth=3, n_estimators=500, subsample=0.9; total time=  11.4s\n",
      "[CV] END learning_rate=0.1, max_depth=4, n_estimators=100, subsample=0.7; total time=   2.4s\n",
      "[CV] END learning_rate=0.1, max_depth=4, n_estimators=100, subsample=0.7; total time=   2.4s\n",
      "[CV] END learning_rate=0.1, max_depth=4, n_estimators=100, subsample=0.7; total time=   2.4s\n",
      "[CV] END learning_rate=0.1, max_depth=4, n_estimators=100, subsample=0.8; total time=   2.7s\n",
      "[CV] END learning_rate=0.1, max_depth=4, n_estimators=100, subsample=0.8; total time=   2.7s\n",
      "[CV] END learning_rate=0.1, max_depth=4, n_estimators=100, subsample=0.8; total time=   2.9s\n",
      "[CV] END learning_rate=0.1, max_depth=4, n_estimators=100, subsample=0.9; total time=   2.9s\n",
      "[CV] END learning_rate=0.1, max_depth=4, n_estimators=100, subsample=0.9; total time=   2.9s\n",
      "[CV] END learning_rate=0.1, max_depth=4, n_estimators=100, subsample=0.9; total time=   2.9s\n",
      "[CV] END learning_rate=0.1, max_depth=4, n_estimators=200, subsample=0.7; total time=   4.8s\n",
      "[CV] END learning_rate=0.1, max_depth=4, n_estimators=200, subsample=0.7; total time=   5.0s\n",
      "[CV] END learning_rate=0.1, max_depth=4, n_estimators=200, subsample=0.7; total time=   4.6s\n",
      "[CV] END learning_rate=0.1, max_depth=4, n_estimators=200, subsample=0.8; total time=   5.0s\n",
      "[CV] END learning_rate=0.1, max_depth=4, n_estimators=200, subsample=0.8; total time=   4.5s\n",
      "[CV] END learning_rate=0.1, max_depth=4, n_estimators=200, subsample=0.8; total time=   4.9s\n",
      "[CV] END learning_rate=0.1, max_depth=4, n_estimators=200, subsample=0.9; total time=   5.5s\n",
      "[CV] END learning_rate=0.1, max_depth=4, n_estimators=200, subsample=0.9; total time=   5.6s\n",
      "[CV] END learning_rate=0.1, max_depth=4, n_estimators=200, subsample=0.9; total time=   5.5s\n",
      "[CV] END learning_rate=0.1, max_depth=4, n_estimators=500, subsample=0.7; total time=  10.9s\n",
      "[CV] END learning_rate=0.1, max_depth=4, n_estimators=500, subsample=0.7; total time=  11.0s\n",
      "[CV] END learning_rate=0.1, max_depth=4, n_estimators=500, subsample=0.7; total time=  11.3s\n",
      "[CV] END learning_rate=0.1, max_depth=4, n_estimators=500, subsample=0.8; total time=  13.0s\n",
      "[CV] END learning_rate=0.1, max_depth=4, n_estimators=500, subsample=0.8; total time=  12.3s\n",
      "[CV] END learning_rate=0.1, max_depth=4, n_estimators=500, subsample=0.8; total time=  12.1s\n",
      "[CV] END learning_rate=0.1, max_depth=4, n_estimators=500, subsample=0.9; total time=  13.6s\n",
      "[CV] END learning_rate=0.1, max_depth=4, n_estimators=500, subsample=0.9; total time=  13.5s\n",
      "[CV] END learning_rate=0.1, max_depth=4, n_estimators=500, subsample=0.9; total time=  13.6s\n",
      "[CV] END learning_rate=0.1, max_depth=5, n_estimators=100, subsample=0.7; total time=   2.3s\n",
      "[CV] END learning_rate=0.1, max_depth=5, n_estimators=100, subsample=0.7; total time=   2.6s\n",
      "[CV] END learning_rate=0.1, max_depth=5, n_estimators=100, subsample=0.7; total time=   2.6s\n",
      "[CV] END learning_rate=0.1, max_depth=5, n_estimators=100, subsample=0.8; total time=   2.9s\n",
      "[CV] END learning_rate=0.1, max_depth=5, n_estimators=100, subsample=0.8; total time=   2.9s\n",
      "[CV] END learning_rate=0.1, max_depth=5, n_estimators=100, subsample=0.8; total time=   3.0s\n",
      "[CV] END learning_rate=0.1, max_depth=5, n_estimators=100, subsample=0.9; total time=   3.3s\n",
      "[CV] END learning_rate=0.1, max_depth=5, n_estimators=100, subsample=0.9; total time=   3.3s\n",
      "[CV] END learning_rate=0.1, max_depth=5, n_estimators=100, subsample=0.9; total time=   3.5s\n",
      "[CV] END learning_rate=0.1, max_depth=5, n_estimators=200, subsample=0.7; total time=   5.3s\n",
      "[CV] END learning_rate=0.1, max_depth=5, n_estimators=200, subsample=0.7; total time=   5.3s\n",
      "[CV] END learning_rate=0.1, max_depth=5, n_estimators=200, subsample=0.7; total time=   5.4s\n",
      "[CV] END learning_rate=0.1, max_depth=5, n_estimators=200, subsample=0.8; total time=   6.0s\n",
      "[CV] END learning_rate=0.1, max_depth=5, n_estimators=200, subsample=0.8; total time=   6.0s\n",
      "[CV] END learning_rate=0.1, max_depth=5, n_estimators=200, subsample=0.8; total time=   5.6s\n",
      "[CV] END learning_rate=0.1, max_depth=5, n_estimators=200, subsample=0.9; total time=   6.7s\n",
      "[CV] END learning_rate=0.1, max_depth=5, n_estimators=200, subsample=0.9; total time=   6.6s\n",
      "[CV] END learning_rate=0.1, max_depth=5, n_estimators=200, subsample=0.9; total time=   6.6s\n",
      "[CV] END learning_rate=0.1, max_depth=5, n_estimators=500, subsample=0.7; total time=  13.5s\n",
      "[CV] END learning_rate=0.1, max_depth=5, n_estimators=500, subsample=0.7; total time=  13.5s\n",
      "[CV] END learning_rate=0.1, max_depth=5, n_estimators=500, subsample=0.7; total time=  13.8s\n",
      "[CV] END learning_rate=0.1, max_depth=5, n_estimators=500, subsample=0.8; total time=  15.4s\n",
      "[CV] END learning_rate=0.1, max_depth=5, n_estimators=500, subsample=0.8; total time=  15.8s\n",
      "[CV] END learning_rate=0.1, max_depth=5, n_estimators=500, subsample=0.8; total time=  14.9s\n",
      "[CV] END learning_rate=0.1, max_depth=5, n_estimators=500, subsample=0.9; total time=  15.8s\n",
      "[CV] END learning_rate=0.1, max_depth=5, n_estimators=500, subsample=0.9; total time=  15.5s\n",
      "[CV] END learning_rate=0.1, max_depth=5, n_estimators=500, subsample=0.9; total time=  17.6s\n",
      "[CV] END learning_rate=0.2, max_depth=3, n_estimators=100, subsample=0.7; total time=   1.7s\n",
      "[CV] END learning_rate=0.2, max_depth=3, n_estimators=100, subsample=0.7; total time=   1.7s\n",
      "[CV] END learning_rate=0.2, max_depth=3, n_estimators=100, subsample=0.7; total time=   1.7s\n",
      "[CV] END learning_rate=0.2, max_depth=3, n_estimators=100, subsample=0.8; total time=   1.9s\n",
      "[CV] END learning_rate=0.2, max_depth=3, n_estimators=100, subsample=0.8; total time=   1.9s\n",
      "[CV] END learning_rate=0.2, max_depth=3, n_estimators=100, subsample=0.8; total time=   1.9s\n",
      "[CV] END learning_rate=0.2, max_depth=3, n_estimators=100, subsample=0.9; total time=   2.1s\n",
      "[CV] END learning_rate=0.2, max_depth=3, n_estimators=100, subsample=0.9; total time=   2.1s\n",
      "[CV] END learning_rate=0.2, max_depth=3, n_estimators=100, subsample=0.9; total time=   2.1s\n",
      "[CV] END learning_rate=0.2, max_depth=3, n_estimators=200, subsample=0.7; total time=   3.4s\n",
      "[CV] END learning_rate=0.2, max_depth=3, n_estimators=200, subsample=0.7; total time=   3.4s\n",
      "[CV] END learning_rate=0.2, max_depth=3, n_estimators=200, subsample=0.7; total time=   3.4s\n",
      "[CV] END learning_rate=0.2, max_depth=3, n_estimators=200, subsample=0.8; total time=   3.9s\n",
      "[CV] END learning_rate=0.2, max_depth=3, n_estimators=200, subsample=0.8; total time=   3.8s\n",
      "[CV] END learning_rate=0.2, max_depth=3, n_estimators=200, subsample=0.8; total time=   3.8s\n",
      "[CV] END learning_rate=0.2, max_depth=3, n_estimators=200, subsample=0.9; total time=   4.2s\n",
      "[CV] END learning_rate=0.2, max_depth=3, n_estimators=200, subsample=0.9; total time=   4.2s\n",
      "[CV] END learning_rate=0.2, max_depth=3, n_estimators=200, subsample=0.9; total time=   4.2s\n",
      "[CV] END learning_rate=0.2, max_depth=3, n_estimators=500, subsample=0.7; total time=   8.9s\n",
      "[CV] END learning_rate=0.2, max_depth=3, n_estimators=500, subsample=0.7; total time=   8.7s\n",
      "[CV] END learning_rate=0.2, max_depth=3, n_estimators=500, subsample=0.7; total time=   8.7s\n",
      "[CV] END learning_rate=0.2, max_depth=3, n_estimators=500, subsample=0.8; total time=   9.8s\n",
      "[CV] END learning_rate=0.2, max_depth=3, n_estimators=500, subsample=0.8; total time=   9.7s\n",
      "[CV] END learning_rate=0.2, max_depth=3, n_estimators=500, subsample=0.8; total time=   9.8s\n",
      "[CV] END learning_rate=0.2, max_depth=3, n_estimators=500, subsample=0.9; total time=  10.6s\n",
      "[CV] END learning_rate=0.2, max_depth=3, n_estimators=500, subsample=0.9; total time=  10.7s\n",
      "[CV] END learning_rate=0.2, max_depth=3, n_estimators=500, subsample=0.9; total time=  10.6s\n",
      "[CV] END learning_rate=0.2, max_depth=4, n_estimators=100, subsample=0.7; total time=   2.2s\n",
      "[CV] END learning_rate=0.2, max_depth=4, n_estimators=100, subsample=0.7; total time=   2.2s\n",
      "[CV] END learning_rate=0.2, max_depth=4, n_estimators=100, subsample=0.7; total time=   2.2s\n",
      "[CV] END learning_rate=0.2, max_depth=4, n_estimators=100, subsample=0.8; total time=   2.5s\n",
      "[CV] END learning_rate=0.2, max_depth=4, n_estimators=100, subsample=0.8; total time=   2.4s\n",
      "[CV] END learning_rate=0.2, max_depth=4, n_estimators=100, subsample=0.8; total time=   2.4s\n",
      "[CV] END learning_rate=0.2, max_depth=4, n_estimators=100, subsample=0.9; total time=   2.7s\n",
      "[CV] END learning_rate=0.2, max_depth=4, n_estimators=100, subsample=0.9; total time=   2.7s\n",
      "[CV] END learning_rate=0.2, max_depth=4, n_estimators=100, subsample=0.9; total time=   2.8s\n",
      "[CV] END learning_rate=0.2, max_depth=4, n_estimators=200, subsample=0.7; total time=   4.6s\n",
      "[CV] END learning_rate=0.2, max_depth=4, n_estimators=200, subsample=0.7; total time=   4.4s\n",
      "[CV] END learning_rate=0.2, max_depth=4, n_estimators=200, subsample=0.7; total time=   4.4s\n",
      "[CV] END learning_rate=0.2, max_depth=4, n_estimators=200, subsample=0.8; total time=   4.9s\n",
      "[CV] END learning_rate=0.2, max_depth=4, n_estimators=200, subsample=0.8; total time=   4.9s\n",
      "[CV] END learning_rate=0.2, max_depth=4, n_estimators=200, subsample=0.8; total time=   5.0s\n",
      "[CV] END learning_rate=0.2, max_depth=4, n_estimators=200, subsample=0.9; total time=   5.5s\n",
      "[CV] END learning_rate=0.2, max_depth=4, n_estimators=200, subsample=0.9; total time=   5.5s\n",
      "[CV] END learning_rate=0.2, max_depth=4, n_estimators=200, subsample=0.9; total time=   5.4s\n",
      "[CV] END learning_rate=0.2, max_depth=4, n_estimators=500, subsample=0.7; total time=  11.3s\n",
      "[CV] END learning_rate=0.2, max_depth=4, n_estimators=500, subsample=0.7; total time=  11.1s\n",
      "[CV] END learning_rate=0.2, max_depth=4, n_estimators=500, subsample=0.7; total time=  11.8s\n",
      "[CV] END learning_rate=0.2, max_depth=4, n_estimators=500, subsample=0.8; total time=  12.5s\n",
      "[CV] END learning_rate=0.2, max_depth=4, n_estimators=500, subsample=0.8; total time=  12.6s\n",
      "[CV] END learning_rate=0.2, max_depth=4, n_estimators=500, subsample=0.8; total time=  12.6s\n",
      "[CV] END learning_rate=0.2, max_depth=4, n_estimators=500, subsample=0.9; total time=  14.4s\n",
      "[CV] END learning_rate=0.2, max_depth=4, n_estimators=500, subsample=0.9; total time=  14.1s\n",
      "[CV] END learning_rate=0.2, max_depth=4, n_estimators=500, subsample=0.9; total time=  14.1s\n",
      "[CV] END learning_rate=0.2, max_depth=5, n_estimators=100, subsample=0.7; total time=   2.7s\n",
      "[CV] END learning_rate=0.2, max_depth=5, n_estimators=100, subsample=0.7; total time=   2.7s\n",
      "[CV] END learning_rate=0.2, max_depth=5, n_estimators=100, subsample=0.7; total time=   2.7s\n",
      "[CV] END learning_rate=0.2, max_depth=5, n_estimators=100, subsample=0.8; total time=   3.1s\n",
      "[CV] END learning_rate=0.2, max_depth=5, n_estimators=100, subsample=0.8; total time=   3.0s\n",
      "[CV] END learning_rate=0.2, max_depth=5, n_estimators=100, subsample=0.8; total time=   3.0s\n",
      "[CV] END learning_rate=0.2, max_depth=5, n_estimators=100, subsample=0.9; total time=   3.3s\n",
      "[CV] END learning_rate=0.2, max_depth=5, n_estimators=100, subsample=0.9; total time=   3.4s\n",
      "[CV] END learning_rate=0.2, max_depth=5, n_estimators=100, subsample=0.9; total time=   3.5s\n",
      "[CV] END learning_rate=0.2, max_depth=5, n_estimators=200, subsample=0.7; total time=   5.7s\n",
      "[CV] END learning_rate=0.2, max_depth=5, n_estimators=200, subsample=0.7; total time=   5.5s\n",
      "[CV] END learning_rate=0.2, max_depth=5, n_estimators=200, subsample=0.7; total time=   5.4s\n",
      "[CV] END learning_rate=0.2, max_depth=5, n_estimators=200, subsample=0.8; total time=   6.3s\n",
      "[CV] END learning_rate=0.2, max_depth=5, n_estimators=200, subsample=0.8; total time=   6.0s\n",
      "[CV] END learning_rate=0.2, max_depth=5, n_estimators=200, subsample=0.8; total time=   6.0s\n",
      "[CV] END learning_rate=0.2, max_depth=5, n_estimators=200, subsample=0.9; total time=   6.8s\n",
      "[CV] END learning_rate=0.2, max_depth=5, n_estimators=200, subsample=0.9; total time=   7.0s\n",
      "[CV] END learning_rate=0.2, max_depth=5, n_estimators=200, subsample=0.9; total time=   6.8s\n",
      "[CV] END learning_rate=0.2, max_depth=5, n_estimators=500, subsample=0.7; total time=  14.1s\n",
      "[CV] END learning_rate=0.2, max_depth=5, n_estimators=500, subsample=0.7; total time=  13.9s\n",
      "[CV] END learning_rate=0.2, max_depth=5, n_estimators=500, subsample=0.7; total time=  13.2s\n",
      "[CV] END learning_rate=0.2, max_depth=5, n_estimators=500, subsample=0.8; total time=  15.1s\n",
      "[CV] END learning_rate=0.2, max_depth=5, n_estimators=500, subsample=0.8; total time=  15.4s\n",
      "[CV] END learning_rate=0.2, max_depth=5, n_estimators=500, subsample=0.8; total time=  15.4s\n",
      "[CV] END learning_rate=0.2, max_depth=5, n_estimators=500, subsample=0.9; total time=  17.0s\n",
      "[CV] END learning_rate=0.2, max_depth=5, n_estimators=500, subsample=0.9; total time=  17.1s\n",
      "[CV] END learning_rate=0.2, max_depth=5, n_estimators=500, subsample=0.9; total time=  16.8s\n",
      "\n",
      "Best parameters found:  {'learning_rate': 0.05, 'max_depth': 3, 'n_estimators': 100, 'subsample': 0.8}\n",
      "Best cross-validation score (RMSE): 0.52\n",
      "\n",
      "Making predictions on the test data with the best model...\n",
      "Predictions generated.\n",
      "\n",
      "Correcting final predictions for submission format...\n",
      "Final predictions saved to 'submission_tuned_v3.csv'!\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# --- Load the Advanced Preprocessed Data ---\n",
    "# Loading the preprocessed training and test data. This code assumes\n",
    "# the files 'preprocessed_train_simplified.csv', 'preprocessed_train_target_simplified.csv',\n",
    "# 'preprocessed_test_simplified.csv', and 'test_AbJTz2l.csv' exist.\n",
    "print(\"Loading preprocessed training and test data...\")\n",
    "try:\n",
    "    # X_train contains the features for training\n",
    "    X_train = pd.read_csv('preprocessed_train_simplified.csv')\n",
    "    # y_train contains the target variable 'Item_Outlet_Sales'\n",
    "    y_train = pd.read_csv('preprocessed_train_target_simplified.csv').iloc[:, 0]\n",
    "    # X_test contains the features for the test set\n",
    "    X_test = pd.read_csv('preprocessed_test_simplified.csv')\n",
    "    # We also need the original test file to get the identifiers for the submission\n",
    "    original_test_identifiers = pd.read_csv('test_AbJTz2l.csv')[['Item_Identifier', 'Outlet_Identifier']]\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Error: Required file not found. Please ensure that 'preprocessed_train_simplified.csv', 'preprocessed_train_target_simplified.csv', 'preprocessed_test_simplified.csv', and 'test_AbJTz2l.csv' are in the directory. Details: {e}\")\n",
    "    exit()\n",
    "\n",
    "# --- Handle NaN Values (just in case) ---\n",
    "# Although preprocessing should handle this, it's a good practice to double-check.\n",
    "X_train.fillna(0, inplace=True)\n",
    "X_test.fillna(0, inplace=True)\n",
    "print(\"Missing values in dataframes filled with 0.\")\n",
    "\n",
    "# --- Log-Transform the Target Variable ---\n",
    "# Log-transforming the target variable to handle skewness, which can lead to\n",
    "# better model performance. We add 1 to the sales to handle potential zero values.\n",
    "print(\"\\nLog-transforming the target variable 'Item_Outlet_Sales'...\")\n",
    "y_train_log = np.log1p(y_train)\n",
    "\n",
    "# --- Hyperparameter Tuning with GridSearchCV ---\n",
    "print(\"\\nStarting hyperparameter tuning with GridSearchCV for GradientBoostingRegressor...\")\n",
    "\n",
    "# Initialize the GradientBoostingRegressor model.\n",
    "gb_reg = GradientBoostingRegressor(random_state=42)\n",
    "\n",
    "# Define a parameter grid to search.\n",
    "# We are tuning a few key parameters:\n",
    "# - n_estimators: The number of boosting stages to perform.\n",
    "# - max_depth: The maximum depth of the individual regression estimators.\n",
    "# - learning_rate: Controls the contribution of each tree.\n",
    "# - subsample: The fraction of samples to be used for fitting the individual base learners.\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 500],\n",
    "    'max_depth': [3, 4, 5],\n",
    "    'learning_rate': [0.05, 0.1, 0.2],\n",
    "    'subsample': [0.7, 0.8, 0.9]\n",
    "}\n",
    "\n",
    "# Set up GridSearchCV.\n",
    "# 'scoring'='neg_mean_squared_error' is used to find the model with the lowest MSE.\n",
    "# 'cv=3' means we will perform 3-fold cross-validation.\n",
    "# We set n_jobs=1 to avoid the multiprocessing error from the previous conversation.\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=gb_reg,\n",
    "    param_grid=param_grid,\n",
    "    scoring='neg_mean_squared_error',\n",
    "    cv=3,\n",
    "    verbose=2,\n",
    "    n_jobs=1\n",
    ")\n",
    "\n",
    "# Fit the grid search to the data. This will train a model for every combination\n",
    "# of parameters in the grid and find the best one.\n",
    "grid_search.fit(X_train, y_train_log)\n",
    "\n",
    "# Print the best parameters and the best score found.\n",
    "print(\"\\nBest parameters found: \", grid_search.best_params_)\n",
    "# The score is negative, so we convert it back to a positive Root Mean Squared Error (RMSE)\n",
    "best_score_rmse = np.sqrt(-grid_search.best_score_)\n",
    "print(f\"Best cross-validation score (RMSE): {best_score_rmse:.2f}\")\n",
    "\n",
    "# --- Make Predictions with the Best Model ---\n",
    "print(\"\\nMaking predictions on the test data with the best model...\")\n",
    "\n",
    "# The best model is stored in the `best_estimator_` attribute of the grid search object.\n",
    "best_gb_model = grid_search.best_estimator_\n",
    "test_predictions_log = best_gb_model.predict(X_test)\n",
    "\n",
    "# Inverse transform the predictions from the log scale back to the original sales scale.\n",
    "test_predictions = np.expm1(test_predictions_log)\n",
    "print(\"Predictions generated.\")\n",
    "\n",
    "# --- Post-processing and Final Output ---\n",
    "# This section ensures the final output is in the correct format for submission.\n",
    "print(\"\\nCorrecting final predictions for submission format...\")\n",
    "\n",
    "# Clip negative values to 0, as sales cannot be negative.\n",
    "test_predictions[test_predictions < 0] = 0\n",
    "\n",
    "# Create the final submission DataFrame.\n",
    "submission_df = pd.DataFrame({\n",
    "    'Item_Identifier': original_test_identifiers['Item_Identifier'],\n",
    "    'Outlet_Identifier': original_test_identifiers['Outlet_Identifier'],\n",
    "    'Item_Outlet_Sales': test_predictions\n",
    "})\n",
    "\n",
    "# Save the predictions to a CSV file.\n",
    "submission_df.to_csv('submission_tuned_v3.csv', index=False)\n",
    "print(\"Final predictions saved to 'submission_tuned_v3.csv'!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cec1b6e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading preprocessed training and test data...\n",
      "Missing values in dataframes filled with 0.\n",
      "\n",
      "Log-transforming the target variable 'Item_Outlet_Sales'...\n",
      "\n",
      "--- Model Performance Comparison with 5-Fold Cross-Validation ---\n",
      "\n",
      "Evaluating Linear Regression...\n",
      "Linear Regression Mean RMSE: 0.54\n",
      "Linear Regression Std Dev RMSE: 0.01\n",
      "\n",
      "Evaluating Ridge...\n",
      "Ridge Mean RMSE: 0.54\n",
      "Ridge Std Dev RMSE: 0.01\n",
      "\n",
      "Evaluating Lasso...\n",
      "Lasso Mean RMSE: 1.02\n",
      "Lasso Std Dev RMSE: 0.02\n",
      "\n",
      "Evaluating Random Forest...\n",
      "Random Forest Mean RMSE: 0.56\n",
      "Random Forest Std Dev RMSE: 0.01\n",
      "\n",
      "Evaluating Gradient Boosting...\n",
      "Gradient Boosting Mean RMSE: 0.52\n",
      "Gradient Boosting Std Dev RMSE: 0.01\n",
      "\n",
      "Evaluating XGBoost...\n",
      "XGBoost Mean RMSE: 0.57\n",
      "XGBoost Std Dev RMSE: 0.01\n",
      "\n",
      "--- Best Model Found ---\n",
      "The best model is Gradient Boosting with a mean cross-validation RMSE of 0.52\n",
      "\n",
      "Training the Gradient Boosting on the entire training dataset...\n",
      "\n",
      "Final predictions saved to 'submission_tuned_v4.csv' in the correct format!\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "\n",
    "# --- Load the Advanced Preprocessed Data ---\n",
    "# Loading the training features, training target, and test features\n",
    "# that were generated by your previous preprocessing script.\n",
    "print(\"Loading preprocessed training and test data...\")\n",
    "try:\n",
    "    # X_train contains the features for training\n",
    "    X_train = pd.read_csv('preprocessed_train_simplified.csv')\n",
    "    # y_train contains the target variable 'Item_Outlet_Sales'\n",
    "    y_train = pd.read_csv('preprocessed_train_target_simplified.csv').iloc[:, 0]\n",
    "    # X_test contains the features for the test set\n",
    "    X_test = pd.read_csv('preprocessed_test_simplified.csv')\n",
    "    # We also need the original test file to get the identifiers for the submission\n",
    "    original_test_identifiers = pd.read_csv('test_AbJTz2l.csv')[['Item_Identifier', 'Outlet_Identifier']]\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Error: Required file not found. Please ensure that 'preprocessed_train_simplified.csv', 'preprocessed_train_target_simplified.csv', 'preprocessed_test_simplified.csv', and 'test_AbJTz2l.csv' are in the directory. Details: {e}\")\n",
    "    exit()\n",
    "\n",
    "# --- Handle NaN Values (just in case) ---\n",
    "# It's always a good practice to ensure there are no missing values before training.\n",
    "X_train.fillna(0, inplace=True)\n",
    "X_test.fillna(0, inplace=True)\n",
    "print(\"Missing values in dataframes filled with 0.\")\n",
    "\n",
    "# --- Log-Transform the Target Variable ---\n",
    "# Log-transforming the target variable to handle its right skewness, which\n",
    "# can help models learn better.\n",
    "print(\"\\nLog-transforming the target variable 'Item_Outlet_Sales'...\")\n",
    "y_train_log = np.log1p(y_train)\n",
    "\n",
    "# --- Define Models for Comparison ---\n",
    "# Here we define a dictionary of models to be compared using cross-validation.\n",
    "# We are using default parameters for a quick and fair comparison.\n",
    "models = {\n",
    "    'Linear Regression': LinearRegression(),\n",
    "    'Ridge': Ridge(random_state=42),\n",
    "    'Lasso': Lasso(random_state=42),\n",
    "    'Random Forest': RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1),\n",
    "    'Gradient Boosting': GradientBoostingRegressor(n_estimators=100, random_state=42),\n",
    "    'XGBoost': xgb.XGBRegressor(objective='reg:squarederror', n_estimators=100, random_state=42)\n",
    "}\n",
    "\n",
    "# --- Perform Cross-Validation and Compare Models ---\n",
    "print(\"\\n--- Model Performance Comparison with 5-Fold Cross-Validation ---\")\n",
    "results = {}\n",
    "for name, model in models.items():\n",
    "    print(f\"\\nEvaluating {name}...\")\n",
    "    try:\n",
    "        # We use cross_val_score with 'neg_mean_squared_error' to get a reliable\n",
    "        # performance metric. The negative sign is because scikit-learn\n",
    "        # uses a scoring convention where higher values are better.\n",
    "        scores = cross_val_score(model, X_train, y_train_log, cv=5, scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "        \n",
    "        # Convert scores to RMSE (Root Mean Squared Error) for better interpretability.\n",
    "        # We take the negative of the scores and then the square root.\n",
    "        rmse_scores = np.sqrt(-scores)\n",
    "        \n",
    "        # Store the mean and standard deviation of the RMSE scores.\n",
    "        results[name] = {'mean_rmse': np.mean(rmse_scores), 'std_rmse': np.std(rmse_scores)}\n",
    "        print(f\"{name} Mean RMSE: {results[name]['mean_rmse']:.2f}\")\n",
    "        print(f\"{name} Std Dev RMSE: {results[name]['std_rmse']:.2f}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during cross-validation for {name}. Details: {e}\")\n",
    "        # Use a placeholder for failed models to prevent script termination.\n",
    "        results[name] = {'mean_rmse': np.inf, 'std_rmse': np.inf}\n",
    "\n",
    "# --- Select the Best Model ---\n",
    "# Find the model with the lowest mean RMSE.\n",
    "best_model_name = min(results, key=lambda k: results[k]['mean_rmse'])\n",
    "best_model_instance = models[best_model_name]\n",
    "print(f\"\\n--- Best Model Found ---\")\n",
    "print(f\"The best model is {best_model_name} with a mean cross-validation RMSE of {results[best_model_name]['mean_rmse']:.2f}\")\n",
    "\n",
    "# --- Final Training and Prediction ---\n",
    "# Train the best-performing model on the full training dataset.\n",
    "print(f\"\\nTraining the {best_model_name} on the entire training dataset...\")\n",
    "best_model_instance.fit(X_train, y_train_log)\n",
    "\n",
    "# Make the final predictions on the test set.\n",
    "test_predictions_log = best_model_instance.predict(X_test)\n",
    "\n",
    "# Inverse transform the predictions from the log scale back to the original sales scale.\n",
    "test_predictions = np.expm1(test_predictions_log)\n",
    "\n",
    "# --- Post-processing and Final Output ---\n",
    "# Clip any negative predictions to 0, as sales cannot be negative.\n",
    "test_predictions[test_predictions < 0] = 0\n",
    "\n",
    "# Create the final submission DataFrame with the correct identifiers.\n",
    "submission_df = pd.DataFrame({\n",
    "    'Item_Identifier': original_test_identifiers['Item_Identifier'],\n",
    "    'Outlet_Identifier': original_test_identifiers['Outlet_Identifier'],\n",
    "    'Item_Outlet_Sales': test_predictions\n",
    "})\n",
    "\n",
    "# Save the predictions to a CSV file.\n",
    "submission_df.to_csv('submission_tuned_v4.csv', index=False)\n",
    "print(\"\\nFinal predictions saved to 'submission_tuned_v4.csv' in the correct format!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7362626e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
